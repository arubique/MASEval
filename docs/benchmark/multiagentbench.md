# MultiAgentBench: Multi-Agent Collaboration Benchmark

The **MultiAgentBench** benchmark evaluates multi-agent collaboration and competition in LLM-based systems across diverse scenarios including research, negotiation, coding, and more.

[MultiAgentBench](https://github.com/ulab-uiuc/MARBLE) (from the MARBLE framework) is designed to evaluate how multiple LLM-based agents collaborate and compete to solve complex tasks. The benchmark features:

- **7 diverse domains**: research, bargaining, coding, database, web, worldsimulation, minecraft
- **Multiple coordination modes**: cooperative, star, tree, hierarchical
- **LLM-based evaluation**: Matches MARBLE's evaluation methodology
- **Framework-agnostic**: Use with any agent framework or MARBLE's native agents

Reference Paper: [MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents](https://arxiv.org/abs/2503.01935)

Check out the [BENCHMARKS.md](https://github.com/parameterlab/MASEval/blob/main/BENCHMARKS.md) file for more information including licenses.

::: maseval.benchmark.multiagentbench.MultiAgentBenchBenchmark

::: maseval.benchmark.multiagentbench.MarbleMultiAgentBenchBenchmark

::: maseval.benchmark.multiagentbench.MultiAgentBenchEnvironment

::: maseval.benchmark.multiagentbench.MultiAgentBenchEvaluator

::: maseval.benchmark.multiagentbench.MarbleAgentAdapter

::: maseval.benchmark.multiagentbench.load_tasks

::: maseval.benchmark.multiagentbench.configure_model_ids

::: maseval.benchmark.multiagentbench.ensure_marble_exists

::: maseval.benchmark.multiagentbench.download_marble

::: maseval.benchmark.multiagentbench.get_domain_info
